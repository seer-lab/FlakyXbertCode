{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a9f90-818c-46c6-a98c-4c75e556ca55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import trax\n",
    "# from trax import layers as tl\n",
    "# import trax.fastmath.numpy as fastnp\n",
    "# from trax.supervised import training\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import random as rnd\n",
    "from random import sample\n",
    "import json\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report , confusion_matrix, roc_auc_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "from scipy.spatial import distance\n",
    "from os import walk\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#initialize codebert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model_codebert = AutoModel.from_pretrained(\"microsoft/codebert-base\").to(device)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba519fdf-c698-498d-893c-53d864148bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import walk\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ce577-e164-455f-a525-1768f3801bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/home/riddhi/MAIN_SDP/Concurrency_bug/MAIN/data/IDoFT_data/IDoFT_dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the relevant fields: 'preprocessed_code' and 'category'\n",
    "relevant_data = data[['preprocessed_code', 'category']]\n",
    "\n",
    "# Filter out the categories to include only 0, 1, 2, 3, and 4\n",
    "filtered_data = relevant_data[relevant_data['category'].isin([0, 1, 2, 3, 4, 5])]\n",
    "\n",
    "# Print the filtered data\n",
    "print(filtered_data)\n",
    "\n",
    "# Process filenames and categories\n",
    "categories = {}\n",
    "\n",
    "# Assuming filenames_v0 and filenames_v12 are not needed, we're directly dealing with 'filename'\n",
    "filename = next(os.walk(\"/home/riddhi/MAIN_SDP/Concurrency_bug/MAIN/data/IDoFT_data/IDoFT_dataset.csv\"), (None, None, []))[2]\n",
    "\n",
    "for file in filename:\n",
    "    label = (file.split(\"@\")[1]).split('.')[0].strip().lower()\n",
    "    if label not in categories.keys():\n",
    "        categories[label] = 0\n",
    "    categories[label] += 1\n",
    "\n",
    "print(\"\\n==> All data : \\n\")\n",
    "for x, y in categories.items():\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44492053-3fa5-45a8-9533-fb7beedd0014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/home/riddhi/MAIN_SDP/Concurrency_bug/MAIN/data/IDoFT_data/IDoFT_dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the relevant fields: 'preprocessed_code' and 'category'\n",
    "relevant_data = data[['preprocessed_code', 'category']]\n",
    "\n",
    "# Filter the data to include only categories 0, 1, 2, 3, and 4\n",
    "filtered_data = relevant_data[relevant_data['category'].isin([0, 1, 2, 3, 4, 5])]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract buggy code and categories for train and validation sets\n",
    "train_buggy_code = train_data['preprocessed_code'].tolist()\n",
    "valid_buggy_code = valid_data['preprocessed_code'].tolist()\n",
    "train_categories = train_data['category'].tolist()\n",
    "valid_categories = valid_data['category'].tolist()\n",
    "\n",
    "# Print the splits for verification\n",
    "print(\"Train buggy code:\", len(train_buggy_code))\n",
    "print(\"Validation buggy code:\", len(valid_buggy_code))\n",
    "print(\"Train categories:\", len(train_categories))\n",
    "print(\"Validation categories:\", len(valid_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabaf50-c824-40f5-b1d2-6cff19b7999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int= {\n",
    "    'OD': 0,\n",
    "    'NIO': 1,\n",
    "    'ID': 2,\n",
    "    'NDOD': 3,\n",
    "    'NOD': 4,\n",
    "    'UD':5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484d8fd-6b01-4aea-8a01-81dfc5371418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, buggy_code, tokenizer, codebert, categories, split):\n",
    "        self.buggy_code = buggy_code\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codebert = codebert.to(device)\n",
    "        self.categories = categories\n",
    "        self.split = split\n",
    "        self.max_len = 0\n",
    "        \n",
    "        # Preprocess categories to map labels to indices\n",
    "        self.label_to_indices = {}\n",
    "        for index, category in enumerate(categories):\n",
    "            if category not in self.label_to_indices:\n",
    "                self.label_to_indices[category] = []\n",
    "            self.label_to_indices[category].append(index)\n",
    "\n",
    "        # For negative sampling\n",
    "        self.labels = categories\n",
    "        self.unique_labels = list(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buggy_code)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train':\n",
    "            # Anchor label\n",
    "            anchor_label = self.categories[idx]\n",
    "\n",
    "            # Positive sample\n",
    "            positive_idx = idx\n",
    "            while positive_idx == idx:  # Ensure different indices for anchor and positive\n",
    "                positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "\n",
    "            # Negative sample\n",
    "            negative_label = random.choice([lab for lab in self.unique_labels if lab != anchor_label])\n",
    "            negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "\n",
    "            anchor = self._process_snippet(self.buggy_code[idx])\n",
    "            positive = self._process_snippet(self.buggy_code[positive_idx])\n",
    "            negative = self._process_snippet(self.buggy_code[negative_idx])\n",
    "            \n",
    "            if len(anchor) <= 768:\n",
    "                short_data = True\n",
    "            else:\n",
    "                short_data = False\n",
    "\n",
    "            return {\n",
    "                'anchor': torch.nn.functional.pad(anchor, (0, 26880 - anchor.size(0))),\n",
    "                'positive': torch.nn.functional.pad(positive, (0, 26880 - positive.size(0))),\n",
    "                'negative': torch.nn.functional.pad(negative, (0, 26880 - negative.size(0))),\n",
    "                'label': anchor_label,  # Optional, depends on how you want to use it\n",
    "                'short_data_flag': short_data\n",
    "            }\n",
    "        elif self.split == 'val':\n",
    "            anchor_label = self.categories[idx]\n",
    "            anchor = self._process_snippet(self.buggy_code[idx])\n",
    "            \n",
    "            return {\n",
    "                'anchor': torch.nn.functional.pad(anchor, (0, 26880 - anchor.size(0))),\n",
    "                'label': anchor_label  # Optional, depends on how you want to use it\n",
    "            }\n",
    "        \n",
    "    def _process_snippet(self, snippet):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Tokenize input without the max_length constraint\n",
    "        inputs = self.tokenizer(snippet, return_tensors='pt').to(device)\n",
    "\n",
    "        if inputs['input_ids'].shape[1] < 512:\n",
    "            with torch.no_grad():\n",
    "                outs = self.codebert(**inputs)\n",
    "            cls = outs.last_hidden_state[:, 0, :].squeeze().detach()\n",
    "            return cls  # Move to CPU to save GPU memory\n",
    "        else:\n",
    "            total_length = inputs['input_ids'].shape[1]\n",
    "            parts = []\n",
    "            step_size = 100\n",
    "            chunk_size = 512\n",
    "\n",
    "            for start_index in range(0, total_length, step_size):\n",
    "                end_index = min(start_index + chunk_size, total_length)\n",
    "                input_segment = {key: val[:, start_index:end_index].to(device) for key, val in inputs.items()}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output_segment = self.codebert(**input_segment)\n",
    "                cls_segment = output_segment.last_hidden_state[:, 0, :].squeeze().detach()\n",
    "                parts.append(cls_segment)  # Move to CPU\n",
    "\n",
    "            concatenated_parts = torch.cat(parts, dim=0)\n",
    "            return concatenated_parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b89d78-363f-4c3a-a263-db4faf2fd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom dataset class\n",
    "train_dataset = SiameseDataset(train_buggy_code, tokenizer, model_codebert, train_categories, 'train')\n",
    "val_dataset = SiameseDataset(valid_buggy_code, tokenizer, model_codebert, valid_categories, 'val')\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Print the lengths of the dataloaders for verification\n",
    "print(\"Train dataloader length:\", len(train_dataloader))\n",
    "print(\"Validation dataloader length:\", len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf15857-574e-482c-a611-a01868ac7b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_triplet_data(data, sample_size_ratio):\n",
    "    \"\"\"\n",
    "    Samples a subset of triplet data (anchor, positive, negative) while preserving class distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: list of dictionaries, where each dictionary has keys 'anchor', 'positive', 'negative', 'label'.\n",
    "    - sample_size_ratio: float, the fraction of the data to sample (0 < sample_size_ratio <= 1).\n",
    "    \n",
    "    Returns:\n",
    "    - sampled_data: list of dictionaries, the sampled subset of the original data.\n",
    "    \"\"\"\n",
    "    # Extract labels to understand the class distribution\n",
    "    labels = [item['label'] for item in data]\n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    sampled_data = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        # Find all items belonging to the current class\n",
    "        class_items = [item for item in data if item['label'] == cls]\n",
    "        # Calculate the number of items to sample from this class\n",
    "        num_samples = int(np.ceil(len(class_items) * sample_size_ratio))\n",
    "        # Randomly sample items without replacement\n",
    "        sampled_items = np.random.choice(class_items, size=num_samples, replace=False)\n",
    "        # Append the sampled items to the output list\n",
    "        sampled_data.extend(sampled_items)\n",
    "    \n",
    "    # Optionally, shuffle the sampled dataset to mix classes\n",
    "    np.random.shuffle(sampled_data)\n",
    "\n",
    "    return sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd5b27-3069-47df-b712-6acc8f5b85f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "plot_data = []\n",
    "plot_labels = []\n",
    "short_data_flags = []\n",
    "for item in tqdm(train_dataset):\n",
    "    \n",
    "    plot_data.append(item['anchor'])\n",
    "    plot_labels.append(item['label'])\n",
    "    short_data_flags.append(item['short_data_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e95823-2083-40ab-b419-b02dc414c97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming plot_data is a list of 1D tensors of different lengths\n",
    "max_length = max(tensor.size(0) for tensor in plot_data)\n",
    "padded_data = [torch.nn.functional.pad(tensor, (0, max_length - tensor.size(0))) for tensor in plot_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a91236-be05-46d0-b434-53e8ae9a2c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673065dd-b188-4a75-92c7-e13e43c19ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = ['skyblue', 'lightgreen', 'salmon', 'gold', 'orchid', 'grey']  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# Create a custom colormap\n",
    "cc = LinearSegmentedColormap.from_list(\"cc\", colors, N=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4c425-13f3-45e4-a5b7-b4547e49fd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X, plot_data, and plot_labels are defined as before\n",
    "# Also assuming bool_values is your list/array of boolean values\n",
    "\n",
    "X = torch.stack(plot_data).cpu().numpy()\n",
    "\n",
    "# Extract labels for each point\n",
    "labelsNums = plot_labels\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "X_embedded = TSNE(n_components=2, init='random').fit_transform(X)\n",
    "\n",
    "# Assuming 'cc' is a colormap, you might need to adjust this to fit your actual colormap variable\n",
    "# Generate an array of colors for each label\n",
    "unique_labels = np.unique(labelsNums)\n",
    "colors = plt.cm.get_cmap(cc, len(unique_labels))  # Adjust 'cc' as needed to your colormap name\n",
    "label_to_color = {label: colors(i / len(unique_labels)) for i, label in enumerate(unique_labels)}\n",
    "colors_array = np.array([label_to_color[label] for label in labelsNums])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Iterate through each datapoint\n",
    "for i in range(len(X_embedded)):\n",
    "    marker = 'x' if short_data_flags[i] else 'o'  # Choose the marker based on the boolean value\n",
    "    plt.scatter(X_embedded[i, 0], X_embedded[i, 1], color=colors_array[i], s=20, alpha=0.6, marker=marker)\n",
    "\n",
    "# Set the background color to black\n",
    "# plt.gca().set_facecolor('white')\n",
    "# # Adjust the color of the ticks and labels for better visibility\n",
    "# plt.tick_params(axis='x', colors='white')\n",
    "# plt.tick_params(axis='y', colors='white')\n",
    "# plt.xlabel('Component 1', color='white')\n",
    "# plt.ylabel('Component 2', color='white')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a5325-7def-42b0-a3a3-7298c8c04cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeBERT tokenizer (adjust the model_name as needed)\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "codebert_model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size, int(embedding_size/2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(embedding_size/2), int(embedding_size/4)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(embedding_size/4), embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f65d80-555a-4967-8265-19ba8dc9f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167eeaca-6023-4ec4-885f-01fc9d00c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "num_epochs = 450\n",
    "embed_size = 26880  # This should match the output size of CodeBERT\n",
    "siamese_network = SiameseNetwork(embed_size).to(device)\n",
    "criterion = TripletLoss(margin=1.0)\n",
    "optimizer = optim.Adam(siamese_network.parameters(), lr=learning_rate)\n",
    "triplet_loss = TripletLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# warmup_Frac = 0.1\n",
    "# total_iter = num_epochs * len(train_dataloader)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, int(warmup_Frac * total_iter), total_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7b516-21fd-4d0c-b967-a7f8fd6f253e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b89e25-f11e-4ceb-828e-43aa9c4a7977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "best_loss = 100.0\n",
    "num_epochs = 1200\n",
    "epoch_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    siamese_network.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        labels = (batch['label']).to(device)\n",
    "        batch_anchor = (batch['anchor']).to(device)\n",
    "        batch_positive = (batch['positive']).to(device)\n",
    "        batch_negative = (batch['negative']).to(device)\n",
    "\n",
    "        anchor_output = siamese_network(batch_anchor)\n",
    "        positive_out = siamese_network(batch_positive)\n",
    "        negative_out = siamese_network(batch_negative)\n",
    "\n",
    "        loss = criterion(anchor_output, positive_out, negative_out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(siamese_network.parameters(), max_norm=2.0)        \n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if total_loss <= best_loss:\n",
    "            # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "            print(\"Loss improved saving model\")\n",
    "            torch.save(siamese_network.state_dict(), 'FlakyXbert_IDoFT_seed123456_epoch1200.pth')\n",
    "            best_loss = total_loss\n",
    "        \n",
    "            \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "    # The line `epoch_loss_list.append(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_dataloader)}\")` is attempting to append a formatted string to a list named `epoch_loss_list`. This string contains information about the current epoch number, total number of epochs, and the average loss calculated for that epoch. However, in the provided code snippet, the `epoch_loss_list` list is not defined or used anywhere else in the code.\n",
    "    epoch_loss_list.append(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss_list[-50:-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8085b9-2c0f-4efc-a826-1c56d4b51a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(siamese_network.state_dict(), 'linear_randomL2_NoOverlap_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c67a7-7394-46cc-b2a9-ed00d8fee7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network.load_state_dict(torch.load('FlakyXbert_IDoFT_seed123456_epoch1200.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f73b7-df72-4391-84f2-1916dbf75d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network.eval() \n",
    "post_train_embed = []\n",
    "post_train_label = []\n",
    "with torch.no_grad():\n",
    "    for item in tqdm(train_dataset):\n",
    "        post_train_embed.append(siamese_network(item['anchor']))\n",
    "        post_train_label.append(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e658fd0-88ec-4643-b738-5e5d8908c753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "X = torch.stack(post_train_embed).cpu()\n",
    "X_np = X.numpy()\n",
    "labelsNums = post_train_label\n",
    "X_embedded= TSNE(n_components=2, init='random').fit_transform(X_np)\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1] , c=labelsNums[:len(X_embedded)], s=10, cmap= cc , alpha=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bda447-8372-4997-979a-bf702ae2a38a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clsembed(snippet):\n",
    "    inputs = tokenizer(snippet, padding='max_length', max_length=218, truncation=True, return_tensors='pt').to(device)\n",
    "    #tokens = [self.tokenizer.cls_token] + inputs  + [self.tokenizer.sep_token]\n",
    "    if(len(inputs)<512):\n",
    "        with torch.no_grad():\n",
    "            outs = model_codebert(**inputs)\n",
    "        cls = outs.last_hidden_state[:, 0, :].squeeze().detach()\n",
    "        return cls\n",
    "    else:\n",
    "        print('longtest')\n",
    "        i = 0\n",
    "        part_vector =  []\n",
    "        while ( i < len(inputs)-200) :\n",
    "            #tokens = [self.tokenizer.cls_token] + inputs[i:i+250] +[self.tokenizer.sep_token]\n",
    "            #tokens_ids= self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_seg = [tokenizer.cls_token] + inputs[i:i+250] +[tokenizer.sep_token]\n",
    "            with torch.no_grad():\n",
    "                ots = model_codebert(**input_seg)\n",
    "            cls = ots.last_hidden_state[:, 0, :].squeeze().detach()\n",
    "            #cls = model_codebert(torch.tensor(inputs)[None,:])[1]\n",
    "            #vector = cls[0].detach().numpy()\n",
    "            parts.append(cls)\n",
    "            i = i+100\n",
    "        return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f34b1b-1577-4ab2-ad41-8d0314224b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "#labels_all = ['async wait','unordered collections','concurrency','time']\n",
    "def get_class_rep(post_train_embed, post_train_label):\n",
    "    # Move each tensor to CPU, convert to numpy, and collect in a list\n",
    "    #post_train_embed = [x.cpu().numpy() for x in post_train_embed]\n",
    "    # If you want to combine them into a single array (assuming they have the same shape)\n",
    "    #post_train_embed = np.concatenate(post_train_embed, axis=0)\n",
    "    representatives = [None] * 6\n",
    "    for label in range(6):\n",
    "        indices = np.where(np.atleast_1d(post_train_label) == label)[0]  # Get the indices as an array\n",
    "        class_vectors = [post_train_embed[i] for i in indices]  # Access each index individually\n",
    "        class_vectors = [x.cpu().numpy() for x in class_vectors]\n",
    "        representatives[label] = np.mean(class_vectors, axis=0)\n",
    "    return representatives\n",
    "\n",
    "def calculate_normalized_distance(vec1, vec2):\n",
    "    # Ensure vec1 and vec2 are numpy arrays\n",
    "    if not isinstance(vec1, np.ndarray):\n",
    "        vec1 = vec1.cpu().detach().numpy()\n",
    "    if not isinstance(vec2, np.ndarray):\n",
    "        vec2 = vec2.cpu().detach().numpy()\n",
    "    \n",
    "    # Normalize each vector to have unit length\n",
    "    norm_vec1 = vec1 / np.linalg.norm(vec1)\n",
    "    norm_vec2 = vec2 / np.linalg.norm(vec2)\n",
    "    \n",
    "    # Calculate Euclidean (L2) distance between the normalized vectors\n",
    "    distance = np.linalg.norm(norm_vec1 - norm_vec2)\n",
    "    \n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7de38-849a-4fb5-bc68-9b2cbfe269ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_to_label={\n",
    "    0: 'OD',\n",
    "    1: 'NIO',\n",
    "    2: 'ID',\n",
    "    3: 'NDOD',\n",
    "    4: 'NOD',\n",
    "    5: 'UD'\n",
    "}\n",
    "\n",
    "\n",
    "def get_closest_cluster(cluster_representatives, projected_vector):\n",
    "    distances = [calculate_normalized_distance(rep, projected_vector) for rep in cluster_representatives]\n",
    "    for i in range(len(distances)):\n",
    "        distances[i] = np.mean(distances[i])\n",
    "    closest_cluster_idx = np.argmin(distances)\n",
    "    return int_to_label[closest_cluster_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fbe07-7b67-4485-976a-8eb8f0691f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = post_train_embed\n",
    "labels = post_train_label\n",
    "def predict(input_vector):\n",
    "    modified_vector = siamese_network(input_vector)\n",
    "    representatives = get_class_rep(embed, labels)\n",
    "    return get_closest_cluster(representatives, modified_vector)\n",
    "    \n",
    "print(predict(train_dataset[10]['anchor']))   \n",
    "    \n",
    "print(int_to_label[train_dataset[10]['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769adf53-42ad-40e3-a2c5-3a46039a2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "# Assuming post_train_embed and post_train_label are defined\n",
    "X = torch.stack(post_train_embed).cpu()\n",
    "X_np = X.numpy()\n",
    "labelsNums = post_train_label\n",
    "\n",
    "# Define vibrant colors for each category (same as used in the bar plot)\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'gold', 'orchid', 'grey']\n",
    "\n",
    "# Ensure labels are within the range of colors [0, 1, 2, 3, 4]\n",
    "# Filter out invalid labels (-1) and keep only valid labels [0, 1, 2, 3, 4]\n",
    "valid_indices = [i for i, label in enumerate(labelsNums) if label in [0, 1, 2, 3, 4, 5]]\n",
    "X_np_filtered = X_np[valid_indices]\n",
    "labelsNums_filtered = [labelsNums[i] for i in valid_indices]\n",
    "\n",
    "# Dimensionality reduction with t-SNE\n",
    "X_embedded = TSNE(n_components=2, init='random').fit_transform(X_np_filtered)\n",
    "\n",
    "# Scatter plot\n",
    "for i, label in enumerate(labelsNums_filtered):\n",
    "    plt.scatter(X_embedded[i, 0], X_embedded[i, 1], color=colors[label], s=10, alpha=1, edgecolor='none', marker='o')\n",
    "\n",
    "# Create legend with category labels\n",
    "legend_labels = ['order-dependent (OD)', 'non-idempotent-outcome (NIO)', 'implementation-dependent (ID)', 'non-deterministic order- dependent (NDOD)', 'non-order-dependent (NOD)', 'unknown dependency (UD)']\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors], labels=legend_labels,bbox_to_anchor=(1.05, 1), \n",
    "                    loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b222a-9cdb-4de4-aff7-49ae9d357e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "\n",
    "# Assuming post_train_embed, post_train_label, and labelsNums are defined\n",
    "X = torch.stack(post_train_embed).cpu()\n",
    "X_np = X.numpy()\n",
    "X_embedded = TSNE(n_components=2, init='random').fit_transform(X_np)\n",
    "\n",
    "# Plot the data points\n",
    "scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labelsNums[:len(X_embedded)], s=10, cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Calculate class representatives\n",
    "representatives = get_class_rep(post_train_embed, post_train_label)  # Make sure this function returns what you expect\n",
    "\n",
    "# Get unique labels and their colors from the scatter plot\n",
    "unique_labels = np.unique(labelsNums[:len(X_embedded)])\n",
    "legend1 = plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "# For each class representative, find the closest point and plot it\n",
    "for i, rep in enumerate(representatives):\n",
    "    if rep is not None:\n",
    "        # Find the closest data point in the original space to this representative\n",
    "        distances = np.linalg.norm(X_np - rep, axis=1)\n",
    "        closest_point_index = np.argmin(distances)\n",
    "        # Use the label of the closest data point to get the correct color\n",
    "        label_of_closest = labelsNums[closest_point_index]\n",
    "        color = scatter.cmap(scatter.norm(label_of_closest))\n",
    "        plt.scatter(X_embedded[closest_point_index, 0], X_embedded[closest_point_index, 1], color=color, edgecolors='red', s=100, marker='X')\n",
    "\n",
    "# You might want to adjust the legend to make sure it correctly represents your data\n",
    "# plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d86a1-f3e7-410b-9eb4-dcc73045fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# # Assuming 'X_embedded' is your t-SNE result and 'labelsNums' are your labels\n",
    "\n",
    "# # Silhouette Score:\n",
    "# # - Measures how similar an object is to its own cluster compared to other clusters.\n",
    "# # - Range: -1 (incorrect clustering) to 1 (highly dense clustering). \n",
    "# #   A score close to 1 means that the clusters are well separated and clearly defined.\n",
    "# silhouette_avg = silhouette_score(X_embedded, labelsNums[:len(X_embedded)])\n",
    "# print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# # Davies-Bouldin Index:\n",
    "# # - Evaluates the clustering quality by measuring the average 'similarity' between each cluster \n",
    "# #   and its most similar one. The similarity is based on a ratio of within-cluster distances to between-cluster distances.\n",
    "# # - Range: 0 to +∞. Lower scores indicate better clustering quality.\n",
    "# davies_bouldin = davies_bouldin_score(X_embedded, labelsNums[:len(X_embedded)])\n",
    "# print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
    "\n",
    "# # Calinski-Harabasz Index:\n",
    "# # - Measures the cluster validity based on the ratio between the within-cluster dispersion and the between-cluster dispersion.\n",
    "# # - Range: Higher values indicate better clustering quality, with no upper limit. Low values indicate clusters with high overlap.\n",
    "# calinski_harabasz = calinski_harabasz_score(X_embedded, labelsNums[:len(X_embedded)])\n",
    "# print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb278b-9a32-458a-ad5d-e1029f424957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "int_to_label={\n",
    "    0: 'OD',\n",
    "    1: 'NIO',\n",
    "    2: 'ID',\n",
    "    3: 'NDOD',\n",
    "    4: 'NOD',\n",
    "    5: 'UD'\n",
    "}\n",
    "\n",
    "label_to_int = {v: k for k, v in int_to_label.items()}  # Create reverse mapping\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Assuming val_dataset is iterable and has 'anchor' and 'label' keys\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for item in val_dataset:\n",
    "    count += 1\n",
    "    input_vector = item['anchor']  # Your input vector for prediction\n",
    "    \n",
    "    # Get predicted cluster/label\n",
    "    predicted_label_str = predict(input_vector)\n",
    "    \n",
    "    # Ensure predicted_label is in the valid range\n",
    "    if predicted_label_str in label_to_int:\n",
    "        predicted_label = label_to_int[predicted_label_str]\n",
    "        predicted_labels.append(predicted_label)\n",
    "    else:\n",
    "        print(f\"Warning: Encountered unknown predicted label {predicted_label_str}\")\n",
    "        continue\n",
    "\n",
    "    # True label\n",
    "    true_label_int = int(item['label'])  # Assuming 'label' contains the true label as integer\n",
    "    if true_label_int in int_to_label:\n",
    "        true_labels.append(true_label_int)\n",
    "    else:\n",
    "        print(f\"Warning: Encountered unknown true label {true_label_int}\")\n",
    "        continue\n",
    "\n",
    "    print(count, \"Predicted:\", predicted_label, \"True:\", true_label_int)\n",
    "\n",
    "# Filter out any invalid entries where predicted or true labels are missing\n",
    "valid_indices = [i for i in range(len(true_labels)) if true_labels[i] in int_to_label and predicted_labels[i] in int_to_label]\n",
    "filtered_true_labels = [true_labels[i] for i in valid_indices]\n",
    "filtered_predicted_labels = [predicted_labels[i] for i in valid_indices]\n",
    "\n",
    "print(\"Loop completed\")\n",
    "\n",
    "# Calculate F1 Score, ignoring unknown labels\n",
    "f1 = f1_score(filtered_true_labels, filtered_predicted_labels, average='weighted', zero_division=0)  # Adjust 'average' as necessary\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c254a-151d-407c-ae6e-9ae13b457340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def multiclass_roc_auc_score(truth, pred, average=\"weighted\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(truth)\n",
    "    truth = lb.transform(truth)\n",
    "    pred = lb.transform(pred)\n",
    "    return roc_auc_score(truth, pred, average=average)\n",
    "\n",
    "# Assuming predicted_labels and true_labels are already defined\n",
    "predicted = predicted_labels\n",
    "labels = true_labels\n",
    "\n",
    "# Filter out 'unknown' labels\n",
    "valid_indices = [i for i, label in enumerate(labels) if label != 'unknown']\n",
    "filtered_labels = [labels[i] for i in valid_indices]\n",
    "filtered_predicted = [predicted[i] for i in valid_indices]\n",
    "\n",
    "unique_labels = list(set(filtered_labels))\n",
    "print(classification_report(filtered_labels, filtered_predicted))\n",
    "\n",
    "print('\\n - Accuracy : ', np.round(metrics.accuracy_score(filtered_labels, filtered_predicted), 2))\n",
    "print(' - Precision : ', np.round(metrics.precision_score(filtered_labels, filtered_predicted, average='weighted'), 2))\n",
    "print(' - Recall : ', np.round(metrics.recall_score(filtered_labels, filtered_predicted, average='weighted'), 2))\n",
    "print(' - F1 score : ', np.round(metrics.f1_score(filtered_labels, filtered_predicted, average='weighted'), 2))\n",
    "print(' - MCC : ', np.round(metrics.matthews_corrcoef(filtered_labels, filtered_predicted), 2))\n",
    "print(' - AUC : ', np.round(multiclass_roc_auc_score(filtered_labels, filtered_predicted), 2))\n",
    "\n",
    "print(\"\\n\\nPerformances by categories\\n\")\n",
    "\n",
    "ind = np.arange(len(unique_labels)) \n",
    "width = 0.35\n",
    "fig, ax = plt.subplots()\n",
    "precision = metrics.precision_recall_fscore_support(filtered_labels, filtered_predicted, labels=unique_labels)[0]\n",
    "recall = metrics.precision_recall_fscore_support(filtered_labels, filtered_predicted, labels=unique_labels)[1]\n",
    "ax.barh(ind - width/2, precision, width, label='Precision')\n",
    "ax.barh(ind + width/2, recall, width, label='Recall')\n",
    "ax.set(yticks=ind, yticklabels=np.array(unique_labels), ylim=[2*width - 1, len(ind)])\n",
    "plt.xlim(0, 1)\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_ylabel(\"Performances\")\n",
    "ax.set_xlabel(\"Categories\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\nConfusion Matrix \")\n",
    "\n",
    "mat = confusion_matrix(filtered_labels, filtered_predicted, labels=unique_labels)\n",
    "df_cm = pd.DataFrame(mat, index=[i for i in unique_labels], columns=[i for i in unique_labels])\n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.heatmap(df_cm / np.sum(df_cm), annot=True, fmt='.3%', cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming you have the predicted and true labels as per your previous code\n",
    "# predicted_labels and true_labels should be lists of labels\n",
    "int_to_label={\n",
    "    0: 'OD',\n",
    "    1: 'NIO',\n",
    "    2: 'ID',\n",
    "    3: 'NDOD',\n",
    "    4: 'NOD',\n",
    "    5: 'UD'\n",
    "}\n",
    "\n",
    "# Shortened category names\n",
    "shortened_labels = {\n",
    "    'OD': 'OD',\n",
    "    'NIO': 'NIO',\n",
    "    'ID': 'ID',\n",
    "    'NDOD': 'NDOD',\n",
    "    'NOD': 'NOD',\n",
    "    'UD': 'UD'\n",
    "}\n",
    "\n",
    "# Filter out invalid labels from both true and predicted labels\n",
    "valid_indices = [i for i in range(len(true_labels)) if true_labels[i] in int_to_label.keys() and predicted_labels[i] in int_to_label.keys()]\n",
    "filtered_true_labels = [true_labels[i] for i in valid_indices]\n",
    "filtered_predicted_labels = [predicted_labels[i] for i in valid_indices]\n",
    "\n",
    "# Calculate F1 scores for each category\n",
    "f1_scores = []\n",
    "categories = list(int_to_label.keys())\n",
    "for category in categories:\n",
    "    true_bin = [1 if label == category else 0 for label in filtered_true_labels]\n",
    "    pred_bin = [1 if label == category else 0 for label in filtered_predicted_labels]\n",
    "    f1 = f1_score(true_bin, pred_bin, zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Define vibrant colors for each category\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'gold', 'orchid', 'grey']\n",
    "\n",
    "# Create bar plot with shortened category names and vibrant colors\n",
    "shortened_categories = [shortened_labels[int_to_label[category]] for category in categories]\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(shortened_categories, f1_scores, color=colors)\n",
    "\n",
    "# Add F1 scores on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 0),  \n",
    "                textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Categories')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Scores by Category')\n",
    "\n",
    "# Display the plot\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfdbdb6-f97c-4d63-a64f-48e3f5a76ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_path = 'test_data_predictions.csv'\n",
    "\n",
    "# # Load the compressed CSV file into a DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Extract the 'TrueLabel' and 'PredictedLabel' columns into separate variables\n",
    "# labels_flaky = df['TrueLabel'].tolist()\n",
    "# predicted_flaky = df['PredictedLabel'].tolist()\n",
    "\n",
    "# print (classification_report(labels, predicted))\n",
    "\n",
    "# print('\\n - Accuracy : ' , np.round( metrics.accuracy_score(labels_flaky,  predicted_flaky) , 2))\n",
    "# print(' - Precision : ' , np.round( metrics.precision_score(labels_flaky,  predicted_flaky , average='weighted') , 2))\n",
    "# print(' - Recall : ' , np.round( metrics.recall_score(labels_flaky,  predicted_flaky , average='weighted') , 2))\n",
    "# print(' - F1 score : ' , np.round( metrics.f1_score(labels_flaky,  predicted_flaky , average='weighted') , 2))\n",
    "# print(' - MCC : ' , np.round( metrics.matthews_corrcoef(labels_flaky,  predicted_flaky) , 2))\n",
    "# print(' - AUC : ' , np.round( multiclass_roc_auc_score(labels_flaky,  predicted_flaky),2) )\n",
    "\n",
    "# print(\"\\n\\nPerfomnaces by categories\\n\")\n",
    "\n",
    "\n",
    "# ind = np.arange(len(unique_labels)) \n",
    "# width = 0.35\n",
    "# fig, ax = plt.subplots()\n",
    "# precision = metrics.precision_recall_fscore_support(labels_flaky,  predicted_flaky ,  labels=unique_labels )[0]\n",
    "# recall = metrics.precision_recall_fscore_support(labels_flaky,  predicted_flaky ,  labels=unique_labels )[1]\n",
    "# ax.barh(ind - width/2, precision, width, label='Precision')\n",
    "# ax.barh(ind + width/2, recall, width, label='Recall')\n",
    "# ax.set(yticks=ind + width, yticklabels=np.array(unique_labels),\n",
    "# ylim=[2*width - 1, len(ind)])\n",
    "# plt.xlim(0,1)\n",
    "# ax.legend(loc='upper right')\n",
    "# ax.set_xlabel(\"Performances\")\n",
    "# ax.set_ylabel(\"Categories\")\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\n\\nConfusion Matrix \")\n",
    "\n",
    "# mat = confusion_matrix(labels_flaky,  predicted_flaky, labels=unique_labels)\n",
    "# df_cm = pd.DataFrame(mat, index = [i for i in unique_labels], columns = [i for i in unique_labels])\n",
    "# plt.figure(figsize = (10,8))\n",
    "# sn.heatmap(df_cm/np.sum(df_cm), annot=True, fmt='.3%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebf722-fc1a-4768-b4fa-9d998c33542b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn import metrics\n",
    "\n",
    "# precision_our_model = metrics.precision_recall_fscore_support(labels, predicted, labels=unique_labels)[0]\n",
    "# recall_our_model = metrics.precision_recall_fscore_support(labels, predicted, labels=unique_labels)[1]\n",
    "# # And assuming 'unique_labels' is defined as before\n",
    "\n",
    "# ind = np.arange(len(unique_labels))  # the x locations for the groups\n",
    "# width = 0.2  # the width of the bars\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(14, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# # Using a set of contrasting, darker colors\n",
    "# colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "# # Plotting Precision and Recall for Your Model\n",
    "# rects1 = ax.bar(ind - width*1.5, precision_our_model, width, label='Our Precision', color=colors[0])\n",
    "# rects2 = ax.bar(ind - width/2, recall_our_model, width, label='Our Recall', color=colors[1])\n",
    "\n",
    "# # Plotting Precision and Recall for Flaky Model\n",
    "# rects3 = ax.bar(ind + width/2, precision_flaky, width, label='FlakyCat Precision', color=colors[2])\n",
    "# rects4 = ax.bar(ind + width*1.5, recall_flaky, width, label='FlakyCat Recall', color=colors[3])\n",
    "\n",
    "# # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Scores')\n",
    "# ax.set_title('Class-wise Precision and Recall for Our Model vs. FlakyCat Model')\n",
    "# ax.set_xticks(ind)\n",
    "# ax.set_xticklabels(unique_labels, rotation=45, ha=\"right\")  # Rotate for better label visibility\n",
    "# ax.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Place the legend outside the figure\n",
    "\n",
    "# # Optional: Adding scores above bars\n",
    "# ax.bar_label(rects1, padding=3, fmt='%.2f')\n",
    "# ax.bar_label(rects2, padding=3, fmt='%.2f')\n",
    "# ax.bar_label(rects3, padding=3, fmt='%.2f')\n",
    "# ax.bar_label(rects4, padding=3, fmt='%.2f')\n",
    "\n",
    "# fig.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the rect to make space for the legend outside\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f921c81-ca68-4042-b8d3-9977de72766b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming metrics have been calculated and stored in variables for both models.\n",
    "# # Example metric variables: accuracy, precision, recall, f1_score, mcc, auc for the original model\n",
    "# # And similarly named variables with a '_flaky' suffix for the comparison model.\n",
    "\n",
    "# metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'MCC', 'AUC']\n",
    "# your_model_metrics = [\n",
    "#     np.round(metrics.accuracy_score(labels, predicted), 2),\n",
    "#     np.round(metrics.precision_score(labels, predicted, average='weighted'), 2),\n",
    "#     np.round(metrics.recall_score(labels, predicted, average='weighted'), 2),\n",
    "#     np.round(metrics.f1_score(labels, predicted, average='weighted'), 2),\n",
    "#     np.round(metrics.matthews_corrcoef(labels, predicted), 2),\n",
    "#     np.round(multiclass_roc_auc_score(labels, predicted), 2)\n",
    "# ]\n",
    "\n",
    "# flaky_model_metrics = [\n",
    "#     np.round(metrics.accuracy_score(labels_flaky, predicted_flaky), 2),\n",
    "#     np.round(metrics.precision_score(labels_flaky, predicted_flaky, average='weighted'), 2),\n",
    "#     np.round(metrics.recall_score(labels_flaky, predicted_flaky, average='weighted'), 2),\n",
    "#     np.round(metrics.f1_score(labels_flaky, predicted_flaky, average='weighted'), 2),\n",
    "#     np.round(metrics.matthews_corrcoef(labels_flaky, predicted_flaky), 2),\n",
    "#     np.round(multiclass_roc_auc_score(labels_flaky, predicted_flaky), 2)\n",
    "# ]\n",
    "\n",
    "# x = np.arange(len(metrics_labels))  # the label locations\n",
    "# width = 0.35  # the width of the bars\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# rects1 = ax.bar(x - width/2, your_model_metrics, width, label='Our Model')\n",
    "# rects2 = ax.bar(x + width/2, flaky_model_metrics, width, label='FlakyCat Model')\n",
    "\n",
    "# # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Scores')\n",
    "# ax.set_title('Metrics Comparison between Our Model and FlakyCat Model')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(metrics_labels)\n",
    "# ax.legend()\n",
    "\n",
    "# ax.bar_label(rects1, padding=3)\n",
    "# ax.bar_label(rects2, padding=3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# def extract_projections(siamese_network, dataloader):\n",
    "#     projections = []\n",
    "#     labels = []\n",
    "#     for batch in dataloader:\n",
    "#         label = batch[\"label\"]\n",
    "#         anchor = batch[\"anchor\"]\n",
    "#         projection = siamese_network(anchor)\n",
    "        \n",
    "#         projections.append(projection.cpu().detach().numpy())\n",
    "#         labels.append(label.numpy())\n",
    "#     projections = np.vstack(projections)\n",
    "#     labels = np.hstack(labels)\n",
    "#     return projections, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projections, labels = extract_projections(siamese_network, train_dataloader)\n",
    "# val_projections, val_labels = extract_projections(siamese_network, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(projections), len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_classifier.fit(projections, labels)\n",
    "\n",
    "# # Train SVM Classifier\n",
    "# svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "# svm_classifier.fit(projections, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Random Forest Classifier\n",
    "# rf_predictions = rf_classifier.predict(val_projections)\n",
    "# rf_accuracy = accuracy_score(val_labels, rf_predictions)\n",
    "# rf_f1 = f1_score(val_labels, rf_predictions, average='weighted')\n",
    "# print(f\"Random Forest Classifier Accuracy: {rf_accuracy}, F1: {rf_f1}\")\n",
    "\n",
    "# # Evaluate SVM Classifier\n",
    "# svm_predictions = svm_classifier.predict(val_projections)\n",
    "# svm_accuracy = accuracy_score(val_labels, svm_predictions)\n",
    "# svm_f1 = f1_score(val_labels, svm_predictions, average='weighted')\n",
    "# print(f\"SVM Classifier Accuracy: {svm_accuracy}, F1:{svm_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdp_venv",
   "language": "python",
   "name": "sdp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
